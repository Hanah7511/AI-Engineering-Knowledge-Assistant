"""
ELITE PRODUCTION RAG PIPELINE - AI ENGINEERING KNOWLEDGE ASSISTANT
==================================================================================
üöÄ RECRUITER-READY FEATURES:
- ‚úÖ Retry Logic with Exponential Backoff (Production Reliability)
- ‚úÖ Safe Fallback (Context-Based, No Extra LLM Cost)
- ‚úÖ Hybrid Architecture (Local Embeddings + OpenAI LLM)
- ‚úÖ Retriever Mode Switch (Similarity + MMR + Hybrid)
- ‚úÖ Source File Filtering (Enterprise Scale)
- ‚úÖ Modern LCEL Runnable Chain (Latest LangChain 2024+)
- ‚úÖ Comprehensive Business Metrics & ROI Tracking
- ‚úÖ Query Caching System (Performance Optimization)
- ‚úÖ A/B Testing Framework (Data-Driven Decisions)
- ‚úÖ Batch Processing (Scalability)
- ‚úÖ Export to JSON/CSV (Data Engineering)
- ‚úÖ Laptop Safe (8GB RAM Optimized)
==================================================================================
Author: AI Engineering Portfolio Project
Architecture: Production-Grade Hybrid RAG with Advanced Features
"""

import os
import time
import json
import csv
import pickle
import hashlib
import re
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from datetime import datetime
from functools import lru_cache
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Load environment variables
load_dotenv()

# Modern LangChain Imports (LCEL Architecture)
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.callbacks.manager import get_openai_callback


# Production Features
from tenacity import (
    retry, 
    stop_after_attempt, 
    wait_exponential, 
    retry_if_exception_type,
    before_sleep_log
)
import logging

# Local Knowledge Base
from src.ingestion.knowledge_base import MLOpsKnowledgeBase

# Optional Rich Display
try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.syntax import Syntax
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    from rich.live import Live
    from rich.layout import Layout
    from rich import print as rprint
    USE_RICH = True
    console = Console()
except ImportError:
    USE_RICH = False
    console = None

# Setup logging for retry attempts
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


@dataclass
class QueryMetrics:
    """
    Comprehensive Business Metrics for RAG System
    Tracks performance, cost, quality, and reliability
    """
    query: str
    total_time: float
    retrieval_time: float
    generation_time: float
    tokens_used: int
    cost_usd: float
    num_sources: int
    confidence_score: float
    retriever_mode: str
    fallback_used: bool
    retry_attempts: int
    cache_hit: bool
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return data
    
    def __repr__(self) -> str:
        return (f"QueryMetrics(time={self.total_time:.3f}s, "
                f"cost=${self.cost_usd:.5f}, confidence={self.confidence_score:.2%})")


class EnhancedProductionRAG:
    """
    üéì AI ENGINEERING KNOWLEDGE ASSISTANT - ELITE PRODUCTION RAG
    
    Architecture:
    - Hybrid: Local embeddings (cost-free) + OpenAI LLM (pay-per-use)
    - Modern LCEL Runnable Chain (LangChain 2024+ standard)
    - Multi-mode retrieval with dynamic switching
    - Enterprise-grade reliability with retry + fallback
    - Comprehensive business metrics & cost tracking
    
    Perfect for: Final Year Projects, Portfolio, Job Interviews
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize Enhanced Production RAG System
        
        Args:
            config: Optional configuration override
        """
        self._print_banner()
        
        # Production-Grade Configuration (Laptop Safe + Cost Optimized)
        self.config = {
            # Retrieval Settings
            "retriever_k": 4,                    # Number of docs to retrieve
            "default_retriever_mode": "similarity",  # similarity, mmr, hybrid
            
            # LLM Settings (Cost Optimized)
            "llm_model": "gpt-4o-mini",         # Cheapest + powerful
            "temperature": 0.1,                  # Professional, consistent
            "max_tokens": 500,                   # Cost control
            
            # Retry & Reliability
            "max_retry_attempts": 3,             # Production standard
            "retry_min_wait": 2,                 # Exponential backoff start
            "retry_max_wait": 10,                # Exponential backoff max
            "enable_fallback": True,             # Safe degradation
            
            # Performance Optimization
            "enable_caching": True,              # Cache identical queries
            "cache_ttl_seconds": 3600,           # 1 hour cache
            "enable_source_filtering": True,     # Enterprise feature
            
            # Business Features
            "enable_cost_tracking": True,        # Track every penny
            "enable_confidence_scoring": True,   # Quality metrics
            "enable_ab_testing": False,          # KEEP FALSE for laptop safety
            
            # Display Settings
            "max_sources_display": 3,            # UI clarity
            "verbose": True,                      # Show detailed info
            
            # Source Filtering (Enterprise)
            "allowed_extensions": [".py", ".md", ".txt", ".json", ".yml", ".yaml"],
            "blocked_sources": [],               # Block specific files
        }
        
        # Override with user config
        if config:
            self.config.update(config)
        
        # Initialize system components
        self._initialize_knowledge_base()
        self._initialize_retrievers()
        self._initialize_llm()
        self._initialize_prompt()
        self._initialize_chain()  # FIX 1 APPLIED HERE
        
        # State Management
        self.query_history: List[QueryMetrics] = []
        self.active_retriever_mode = self.config["default_retriever_mode"]
        
        # Cache System
        self.cache: Dict[str, Dict] = {}
        self.cache_file = "data/query_cache.pkl"
        if self.config["enable_caching"]:
            self._load_cache()
        
        # Print initialization summary
        self._print_initialization_complete()
    
    def _print_banner(self):
        """Print system banner"""
        if USE_RICH:
            banner = """
[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]
[bold cyan]‚ïë[/bold cyan]  üéì AI ENGINEERING KNOWLEDGE ASSISTANT - PRODUCTION RAG  [bold cyan]‚ïë[/bold cyan]
[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]
            """
            console.print(banner)
        else:
            print("\n" + "="*60)
            print("üéì AI ENGINEERING KNOWLEDGE ASSISTANT - PRODUCTION RAG")
            print("="*60)
    
    def _initialize_knowledge_base(self):
        """Load vector knowledge base with error handling"""
        try:
            if self.config["verbose"]:
                print("üìö Loading Knowledge Base...")
            
            self.kb = MLOpsKnowledgeBase()
            self.kb.load_vector_store()
            self.kb_ready = True
            
            if self.config["verbose"]:
                print("‚úÖ Knowledge Base Loaded Successfully")
                
        except Exception as e:
            logger.error(f"Failed to load knowledge base: {e}")
            self.kb_ready = False
            print(f"‚ùå Knowledge Base Load Failed: {e}")
            print("   Please ensure vector store is initialized first.")
    
    def _initialize_retrievers(self):
        """
        Initialize multiple retriever modes
        - Similarity: Fast, direct semantic search
        - MMR: Diversity-optimized (reduces redundancy)
        - Hybrid: Combines multiple strategies
        """
        if not self.kb_ready:
            self.retrievers = {}
            return
        
        try:
            if self.config["verbose"]:
                print("üîç Initializing Retrieval Modes...")
            
            # Similarity Retriever (Fast & Direct)
            self.retrievers = {
                "similarity": self.kb.get_retriever(
                    k=self.config["retriever_k"],
                    search_type="similarity"
                ),
                
                # MMR Retriever (Diversity Optimization)
                "mmr": self.kb.get_retriever(
                    k=self.config["retriever_k"],
                    search_type="mmr"
                ),
            }
            
            
            
            # Set active retriever
            self.retriever = self.retrievers[self.active_retriever_mode]
            
            if self.config["verbose"]:
                print(f"‚úÖ Retrievers Initialized: {list(self.retrievers.keys())}")
                print(f"   Active Mode: {self.active_retriever_mode}")
                
        except Exception as e:
            logger.error(f"Retriever initialization failed: {e}")
            self.retrievers = {}
    
    def _initialize_llm(self):
        """Initialize OpenAI LLM with cost-optimized settings"""
        try:
            if self.config["verbose"]:
                print("ü§ñ Initializing Language Model...")
            
            self.llm = ChatOpenAI(
                model=self.config["llm_model"],
                temperature=self.config["temperature"],
                max_tokens=self.config["max_tokens"],
                streaming=False,  # For callback tracking
            )
            
            if self.config["verbose"]:
                print(f"‚úÖ LLM Ready: {self.config['llm_model']}")
                
        except Exception as e:
            logger.error(f"LLM initialization failed: {e}")
            raise
    
    def _initialize_prompt(self):
        """
        Create production-grade prompt template
        - Strict anti-hallucination rules
        - Source citation requirements
        - Professional tone
        """
        template = """You are an elite AI Engineering Knowledge Assistant for internal technical documentation.

‚ö° STRICT PRODUCTION RULES:
1. Answer ONLY using the provided CONTEXT below
2. DO NOT hallucinate or invent any information
3. ALWAYS cite sources using [filename] format after relevant statements
4. Be technical, precise, and professional
5. If the answer is NOT in the context, respond EXACTLY:
   "‚ùå Information not found in available documentation."
6. Include code snippets from context with proper formatting
7. Prioritize recent and authoritative sources

üìö CONTEXT:
{context}

‚ùì QUESTION:
{question}

üí° PROFESSIONAL ANSWER (with [source] citations):"""
        
        self.prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )
        
        if self.config["verbose"]:
            print("‚úÖ Prompt Template Configured")
    
    # ==================== FIX 1: DYNAMIC RETRIEVAL ====================
    
    def _dynamic_retrieval(self, question: str) -> List[Document]:
        """
        Dynamic retriever wrapper (production best practice)
        Allows runtime switching without rebuilding entire chain
        
        Args:
            question: User query
            
        Returns:
            Filtered documents
        """
        docs = self.retriever.get_relevant_documents(question)
        
        if self.config["enable_source_filtering"]:
            docs = self._filter_sources(docs)
        
        return docs
    
    def _format_docs(self, docs: List[Document]) -> str:
        """
        Format documents for context with source tracking
        
        Args:
            docs: Retrieved documents
            
        Returns:
            Formatted context string
        """
        formatted_parts = []
        
        for i, doc in enumerate(docs):
            source = doc.metadata.get("source", "unknown")
            page = doc.metadata.get("page", "")
            
            # Create citation
            citation = f"[{source}]" if not page else f"[{source}, p.{page}]"
            
            # Format content
            content = doc.page_content[:2000]  # Limit token usage
            formatted_parts.append(f"Source {i+1} {citation}:\n{content}\n")
        
        return "\n---\n".join(formatted_parts)
    
    def _filter_sources(self, docs: List[Document]) -> List[Document]:
        """
        üè¢ ENTERPRISE FEATURE: Source File Filtering
        
        Why: In production, you want to:
        - Exclude low-quality sources (logs, configs)
        - Prioritize code and documentation
        - Enforce security policies (exclude sensitive files)
        
        Args:
            docs: Raw retrieved documents
            
        Returns:
            Filtered documents based on enterprise rules
        """
        if not self.config["enable_source_filtering"]:
            return docs
        
        filtered = []
        
        for doc in docs:
            source = doc.metadata.get("source", "").lower()
            
            # Check allowed extensions
            has_allowed_ext = any(
                source.endswith(ext) 
                for ext in self.config["allowed_extensions"]
            )
            
            # Check blocked sources
            is_blocked = any(
                blocked in source 
                for blocked in self.config["blocked_sources"]
            )
            
            if has_allowed_ext and not is_blocked:
                filtered.append(doc)
        
        # Fallback: if all filtered out, return top 3 originals
        return filtered if filtered else docs[:3]
    
    # ==================== FIX 1: MODERN LCEL CHAIN ====================
    
    def _initialize_chain(self):
        """Modern LCEL Chain with Dynamic Retrieval (ELITE FIX)"""
        if not self.kb_ready or not self.retrievers:
            self.chain = None
            return
        
        try:
            self.chain = (
                RunnableParallel(
                    {
                        "context": self._dynamic_retrieval | self._format_docs,
                        "question": RunnablePassthrough()
                    }
                )
                | self.prompt
                | self.llm
                | StrOutputParser()
            )
            
            if self.config["verbose"]:
                print("‚úÖ Dynamic LCEL Chain Built (Retriever Switch Optimized)")
                
        except Exception as e:
            logger.error(f"Chain initialization failed: {e}")
            self.chain = None
    
    def _print_initialization_complete(self):
        """Print initialization summary"""
        if USE_RICH:
            summary = Table(title="üéØ System Configuration", show_header=False)
            summary.add_column("Setting", style="cyan")
            summary.add_column("Value", style="green")
            
            summary.add_row("Knowledge Base", "‚úÖ Ready" if self.kb_ready else "‚ùå Failed")
            summary.add_row("LLM Model", self.config["llm_model"])
            summary.add_row("Retriever Mode", self.active_retriever_mode.upper())
            summary.add_row("Cost Tracking", "‚úÖ Enabled")
            summary.add_row("Query Caching", "‚úÖ Enabled" if self.config["enable_caching"] else "‚ùå Disabled")
            summary.add_row("Source Filtering", "‚úÖ Enabled" if self.config["enable_source_filtering"] else "‚ùå Disabled")
            summary.add_row("A/B Testing", "‚úÖ Enabled" if self.config["enable_ab_testing"] else "‚ùå Disabled (Laptop Safe)")
            summary.add_row("Retry Logic", f"‚úÖ Max {self.config['max_retry_attempts']} attempts")
            summary.add_row("Architecture", "üöÄ Dynamic LCEL Chain")
            
            console.print(summary)
        else:
            print("\n" + "="*60)
            print("üéØ SYSTEM CONFIGURATION")
            print("="*60)
            print(f"Knowledge Base: {'‚úÖ Ready' if self.kb_ready else '‚ùå Failed'}")
            print(f"LLM Model: {self.config['llm_model']}")
            print(f"Retriever Mode: {self.active_retriever_mode.upper()}")
            print(f"Cost Tracking: ‚úÖ Enabled")
            print(f"Query Caching: {'‚úÖ Enabled' if self.config['enable_caching'] else '‚ùå Disabled'}")
            print(f"Source Filtering: {'‚úÖ Enabled' if self.config['enable_source_filtering'] else '‚ùå Disabled'}")
            print(f"A/B Testing: {'‚úÖ Enabled' if self.config['enable_ab_testing'] else '‚ùå Disabled (Laptop Safe)'}")
            print(f"Retry Logic: ‚úÖ Max {self.config['max_retry_attempts']} attempts")
            print(f"Architecture:  Dynamic LCEL Chain")
    
    # ==================== CACHE SYSTEM ====================
    
    def _get_cache_key(self, question: str) -> str:
        """Generate deterministic cache key"""
        normalized = question.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _load_cache(self):
        """Load cache from disk"""
        try:
            # Create data directory if needed
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    self.cache = pickle.load(f)
                
                # Filter expired entries
                current_time = time.time()
                self.cache = {
                    k: v for k, v in self.cache.items()
                    if current_time - v.get('timestamp', 0) < self.config["cache_ttl_seconds"]
                }
                
                if self.config["verbose"]:
                    print(f"üíæ Cache Loaded: {len(self.cache)} entries")
                    
        except Exception as e:
            logger.warning(f"Cache load failed: {e}")
            self.cache = {}
    
    def _save_cache(self):
        """Save cache to disk"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")
    
    def clear_cache(self):
        """Clear all cached queries"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        print("üßπ Cache cleared successfully")
    
    # ==================== CONFIDENCE SCORING ====================
    
    def _calculate_confidence(self, sources: List[Document], answer: str) -> float:
        """
        Calculate business confidence score (0-1)
        
        Factors:
        1. Number of sources (more = better)
        2. Content quality (length, technical depth)
        3. Source diversity (different files)
        4. Answer specificity (not generic)
        
        Args:
            sources: Retrieved documents
            answer: Generated answer
            
        Returns:
            Confidence score between 0 and 1
        """
        if not self.config["enable_confidence_scoring"]:
            return 0.5
        
        if not sources:
            return 0.0
        
        # Factor 1: Number of sources (0-0.3)
        source_count_score = min(len(sources) / 4, 1.0) * 0.3
        
        # Factor 2: Average content quality (0-0.3)
        avg_length = sum(len(doc.page_content) for doc in sources) / len(sources)
        quality_score = min(avg_length / 1500, 1.0) * 0.3
        
        # Factor 3: Source diversity (0-0.2)
        unique_sources = len(set(doc.metadata.get('source', '') for doc in sources))
        diversity_score = min(unique_sources / 3, 1.0) * 0.2
        
        # Factor 4: Answer specificity (0-0.2)
        has_code = '```' in answer or 'def ' in answer or 'class ' in answer
        has_citations = '[' in answer and ']' in answer
        specificity_score = (0.1 if has_code else 0) + (0.1 if has_citations else 0)
        
        confidence = source_count_score + quality_score + diversity_score + specificity_score
        
        return round(confidence, 3)
    
    # ==================== FIX 2: REAL RETRY ATTEMPT TRACKING ====================
    
    def _safe_llm_invoke_with_tracking(self, question: str):
        """
        Retry logic with attempt tracking (FIX 2)
        
        Args:
            question: User query
            
        Returns:
            Tuple of (result, attempts_count, error)
        """
        attempts = {"count": 0}
        
        @retry(
            stop=stop_after_attempt(self.config["max_retry_attempts"]),
            wait=wait_exponential(
                multiplier=1,
                min=self.config["retry_min_wait"],
                max=self.config["retry_max_wait"]
            ),
            retry=retry_if_exception_type((Exception,)),
            before_sleep=before_sleep_log(logger, logging.WARNING)
        )
        def _invoke():
            attempts["count"] += 1
            if self.config["verbose"] and attempts["count"] > 1:
                print(f"üîÑ Retry attempt {attempts['count']}...")
            return self.chain.invoke(question)
        
        try:
            result = _invoke()
            return result, attempts["count"], None
        except Exception as e:
            return None, attempts["count"], e
    
    # ==================== MAIN QUERY FUNCTION ====================
    
    def ask(
        self, 
        question: str, 
        use_cache: bool = True,
        retriever_mode: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        üéØ MAIN QUERY FUNCTION - The Heart of the System
        
        Features:
        - ‚úÖ Query caching (performance)
        - ‚úÖ Source filtering (quality)
        - ‚úÖ Retry logic (reliability)
        - ‚úÖ Fallback mechanism (availability)
        - ‚úÖ Comprehensive metrics (observability)
        - ‚úÖ Cost tracking (business value)
        
        Args:
            question: User's question
            use_cache: Whether to use cache
            retriever_mode: Override retriever mode (similarity, mmr)
            
        Returns:
            Dict with answer, sources, and metrics
        """
        if not self.kb_ready or not self.chain:
            return {
                "answer": "‚ùå System not ready. Please initialize knowledge base first.",
                "sources": [],
                "metrics": None,
                "error": "System not initialized"
            }
        
        # Temporarily switch retriever if specified
        original_mode = self.active_retriever_mode
        if retriever_mode and retriever_mode in self.retrievers:
            self.switch_retriever_mode(retriever_mode, verbose=False)
        
        # Check cache
        cache_hit = False
        if use_cache and self.config["enable_caching"]:
            cache_key = self._get_cache_key(question)
            if cache_key in self.cache:
                cached = self.cache[cache_key]
                if time.time() - cached.get('timestamp', 0) < self.config["cache_ttl_seconds"]:
                    print("üíæ [CACHE HIT] Returning cached result")
                    
                    # Restore original retriever
                    if retriever_mode:
                        self.switch_retriever_mode(original_mode, verbose=False)
                    
                    return cached['result']
        
        # Display query
        if USE_RICH:
            console.print(f"\n[bold cyan]üîé Question:[/bold cyan] {question}")
        else:
            print(f"\nüîé Question: {question}")
        
        # Start timing
        start_total = time.time()
        
        # Step 1: Retrieval with timing
        start_retrieval = time.time()
        try:
            # Use dynamic retrieval directly for metrics
            retrieved_docs = self._dynamic_retrieval(question)
            retrieval_time = time.time() - start_retrieval
            
        except Exception as e:
            logger.error(f"Retrieval failed: {e}")
            
            # Restore original retriever
            if retriever_mode:
                self.switch_retriever_mode(original_mode, verbose=False)
            
            return {
                "answer": f"‚ùå Retrieval failed: {str(e)}",
                "sources": [],
                "metrics": None,
                "error": str(e)
            }
        
        # Step 2: Generation with FIX 2 retry tracking
        fallback_used = False
        retry_attempts = 0
        
        with get_openai_callback() as cb:
            start_generation = time.time()
            
            # FIX 2: Use the new retry tracking method
            answer, retry_attempts, error = self._safe_llm_invoke_with_tracking(question)
            
            if error:
                if self.config["enable_fallback"]:
                    fallback_used = True
                    answer = self._fallback_response(retrieved_docs, error)
                else:
                    # Restore original retriever
                    if retriever_mode:
                        self.switch_retriever_mode(original_mode, verbose=False)
                    raise error
            
            generation_time = time.time() - start_generation
            total_time = time.time() - start_total
            
            # Calculate confidence
            confidence = self._calculate_confidence(retrieved_docs, answer)
            
            # Create comprehensive metrics
            metrics = QueryMetrics(
                query=question,
                total_time=round(total_time, 3),
                retrieval_time=round(retrieval_time, 3),
                generation_time=round(generation_time, 3),
                tokens_used=cb.total_tokens if not fallback_used else 0,
                cost_usd=round(cb.total_cost, 6) if not fallback_used else 0.0,
                num_sources=len(retrieved_docs),
                confidence_score=confidence,
                retriever_mode=self.active_retriever_mode,
                fallback_used=fallback_used,
                retry_attempts=retry_attempts,  # Now accurate!
                cache_hit=cache_hit
            )
            
            # Store in history
            self.query_history.append(metrics)
        
        # Prepare result
        result = {
            "answer": answer,
            "sources": retrieved_docs,
            "metrics": metrics
        }
        
        # Cache the result
        if self.config["enable_caching"] and not fallback_used:
            cache_key = self._get_cache_key(question)
            self.cache[cache_key] = {
                'result': result,
                'timestamp': time.time()
            }
            self._save_cache()
        
        # Display results
        self._display_results(answer, retrieved_docs, metrics)
        
        # Restore original retriever
        if retriever_mode:
            self.switch_retriever_mode(original_mode, verbose=False)
        
        return result
    
    def _fallback_response(self, docs: List[Document], error: Exception) -> str:
        """
        üõü SAFE FALLBACK (NO EXTRA LLM COST)
        
        When API fails after retries:
        - Return context-based summary
        - No additional LLM calls
        - Maintains system availability
        
        Args:
            docs: Retrieved documents
            error: Original error
            
        Returns:
            Fallback response text
        """
        if not docs:
            return (
                "‚ö†Ô∏è API temporarily unavailable and no relevant context found.\n"
                f"Error: {str(error)}\n"
                "Please try again in a moment."
            )
        
        # Build context-based fallback
        summary_parts = [
            "‚ö†Ô∏è API temporarily unavailable. Showing retrieved context:\n",
            "\nüìö Relevant Documentation Found:\n"
        ]
        
        for i, doc in enumerate(docs[:3], 1):
            source = doc.metadata.get("source", "Unknown")
            preview = doc.page_content[:300].strip()
            summary_parts.append(f"\n{i}. [{source}]\n{preview}...\n")
        
        summary_parts.append(
            "\nüí° Note: This is raw context. For AI-generated answer, please retry."
        )
        
        return "".join(summary_parts)
    
    def _display_results(
        self, 
        answer: str, 
        sources: List[Document], 
        metrics: QueryMetrics
    ):
        """Display results with rich formatting"""
        if USE_RICH:
            self._display_rich_results(answer, sources, metrics)
        else:
            self._display_plain_results(answer, sources, metrics)
    
    def _display_rich_results(
        self, 
        answer: str, 
        sources: List[Document], 
        metrics: QueryMetrics
    ):
        """Rich terminal display"""
        # Answer Panel
        console.print("\n[bold green]üí° AI ANSWER[/bold green]")
        
        # Handle code blocks
        if "```" in answer:
            parts = re.split(r'(```.*?```)', answer, flags=re.DOTALL)
            for part in parts:
                if part.startswith('```') and part.endswith('```'):
                    lines = part.strip('`').split('\n')
                    lang = lines[0] if lines and lines[0] else "python"
                    code = '\n'.join(lines[1:]) if len(lines) > 1 else ''
                    if code:
                        syntax = Syntax(code, lang, theme="monokai", line_numbers=True)
                        console.print(syntax)
                else:
                    if part.strip():
                        console.print(Panel(part.strip(), border_style="green"))
        else:
            console.print(Panel(answer, border_style="green", padding=(1, 2)))
        
        # Sources Table
        if sources:
            console.print("\n[bold magenta]üìö SOURCES REFERENCED[/bold magenta]")
            source_table = Table(show_header=True, header_style="bold blue")
            source_table.add_column("#", style="dim", width=4)
            source_table.add_column("Source", width=35)
            source_table.add_column("Preview", width=55)
            
            for i, doc in enumerate(sources[:self.config["max_sources_display"]], 1):
                source = doc.metadata.get("source", "Unknown")
                page = doc.metadata.get("page", "")
                if page:
                    source = f"{source} (p.{page})"
                
                preview = doc.page_content[:120].replace('\n', ' ') + "..."
                source_table.add_row(str(i), source, preview)
            
            console.print(source_table)
        
        # Metrics Table
        console.print("\n[bold yellow]üìä PERFORMANCE METRICS[/bold yellow]")
        metrics_table = Table(show_header=True, header_style="bold cyan")
        metrics_table.add_column("Metric", style="cyan", width=25)
        metrics_table.add_column("Value", justify="right", width=20)
        
        metrics_table.add_row("‚è±Ô∏è  Total Time", f"{metrics.total_time:.3f}s")
        metrics_table.add_row("üîç Retrieval Time", f"{metrics.retrieval_time:.3f}s")
        metrics_table.add_row("‚ö° Generation Time", f"{metrics.generation_time:.3f}s")
        metrics_table.add_row("üéØ Confidence Score", f"{metrics.confidence_score*100:.1f}%")
        metrics_table.add_row("üìÑ Sources Retrieved", str(metrics.num_sources))
        metrics_table.add_row("üîÑ Retriever Mode", metrics.retriever_mode.upper())
        metrics_table.add_row("üîÑ Retry Attempts", str(metrics.retry_attempts))  # Now accurate!
        
        if not metrics.fallback_used:
            metrics_table.add_row("üí∞ Tokens Used", f"{metrics.tokens_used:,}")
            metrics_table.add_row("üíµ API Cost", f"${metrics.cost_usd:.6f}")
        else:
            metrics_table.add_row("üõü Fallback Used", "Yes")
        
        if metrics.cache_hit:
            metrics_table.add_row("üíæ Cache", "HIT")
        
        console.print(metrics_table)
    
    def _display_plain_results(
        self, 
        answer: str, 
        sources: List[Document], 
        metrics: QueryMetrics
    ):
        """Plain text display"""
        print("\n" + "="*70)
        print("üí° AI ANSWER")
        print("="*70)
        print(answer)
        
        if sources:
            print("\nüìö SOURCES REFERENCED:")
            print("-"*70)
            for i, doc in enumerate(sources[:self.config["max_sources_display"]], 1):
                source = doc.metadata.get("source", "Unknown")
                page = doc.metadata.get("page", "")
                if page:
                    source = f"{source} (p.{page})"
                
                preview = doc.page_content[:100].replace('\n', ' ') + "..."
                print(f"{i}. {source}")
                print(f"   {preview}\n")
        
        print("üìä PERFORMANCE METRICS:")
        print("-"*70)
        print(f"‚è±Ô∏è  Total Time: {metrics.total_time:.3f}s")
        print(f"üîç Retrieval Time: {metrics.retrieval_time:.3f}s")
        print(f"‚ö° Generation Time: {metrics.generation_time:.3f}s")
        print(f"üéØ Confidence Score: {metrics.confidence_score*100:.1f}%")
        print(f"üìÑ Sources Retrieved: {metrics.num_sources}")
        print(f"üîÑ Retriever Mode: {metrics.retriever_mode.upper()}")
        print(f"üîÑ Retry Attempts: {metrics.retry_attempts}")  # Now accurate!
        
        if not metrics.fallback_used:
            print(f"üí∞ Tokens Used: {metrics.tokens_used:,}")
            print(f"üíµ API Cost: ${metrics.cost_usd:.6f}")
        else:
            print("üõü Fallback Used: Yes")
        
        if metrics.cache_hit:
            print("üíæ Cache: HIT")
    
    # ==================== RETRIEVER MODE SWITCHING ====================
    
    def switch_retriever_mode(self, mode: str, verbose: bool = True):
        """
        üîÑ DYNAMIC RETRIEVER SWITCHING
        
        Allows A/B testing and optimization
        
        Args:
            mode: "similarity" or "mmr"
            verbose: Whether to print confirmation
        """
        if mode not in self.retrievers:
            available = list(self.retrievers.keys())
            print(f"‚ùå Invalid mode. Available: {available}")
            return
        
        self.active_retriever_mode = mode
        self.retriever = self.retrievers[mode]
        
        # Chain automatically uses self.retriever via _dynamic_retrieval
        # No need to rebuild chain!
        
        if verbose:
            print(f"üîÑ Switched to {mode.upper()} retriever")
    
    # ==================== BUSINESS ANALYTICS ====================
    
    def calculate_roi(self) -> Dict[str, Any]:
        """
        üí∞ CALCULATE BUSINESS ROI
        
        Shows:
        - Time saved (engineer hours)
        - Cost saved (vs manual search)
        - ROI percentage
        - Breakeven analysis
        
        Returns:
            ROI metrics dictionary
        """
        if not self.query_history:
            return {"message": "No query data available for ROI calculation"}
        
        # Calculate aggregates
        total_queries = len(self.query_history)
        total_api_cost = sum(q.cost_usd for q in self.query_history)
        total_fallback_queries = sum(1 for q in self.query_history if q.fallback_used)
        
        # Assumptions (adjust based on your context)
        minutes_saved